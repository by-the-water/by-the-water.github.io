<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>By the water</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2017-08-02T21:18:00-05:00</updated><entry><title>Traffic sign recognition with ConvNet</title><link href="/posts/2017/08/02/traffic-sign-recognition.html" rel="alternate"></link><published>2017-08-02T21:18:00-05:00</published><updated>2017-08-02T21:18:00-05:00</updated><author><name>By-the-water</name></author><id>tag:None,2017-08-02:/posts/2017/08/02/traffic-sign-recognition.html</id><summary type="html">&lt;p&gt;In this project, a classifier to recognize traffic sign was built based on Convolutional Neural Networks, which is a project of the Udacity Self-driving Cars Nanodegree program. The model was trained on a fraction of the German Traffic Sign dataset, which is provided as pickled files by the Self-driving car program. The final validation accuracy of our model was 97%, and the test accuracy was 98%. Six images downloaded from the internet were also used to test the model, and a accuracy of 66.7% was achieved.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;&lt;strong&gt;Traffic Sign Recognition&lt;/strong&gt;&lt;/h1&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;h3&gt;In this project, a classifier to recognize traffic sign was built based on Convolutional Neural Networks, which is a project of the Udacity Self-driving Cars Nanodegree program. The model was trained on a fraction of the German Traffic Sign dataset, which is provided as pickled files by the Self-driving car program. The final validation accuracy of our model was 97%, and the test accuracy was 98%. Six images downloaded from the internet were also used to test the model, and a accuracy of 66.7% was achieved.&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;The document includes following sections&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data set summary and exploration&lt;/li&gt;
&lt;li&gt;Data augmentation and preprocessing&lt;/li&gt;
&lt;li&gt;Model architecture design&lt;/li&gt;
&lt;li&gt;Make prediction on new images&lt;/li&gt;
&lt;li&gt;Softmax probabilities of the new images&lt;/li&gt;
&lt;li&gt;Visualize feature maps of convolutional layers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data Set Summary &amp;amp; Exploration&lt;/h3&gt;
&lt;p&gt;The provided dataset was explored with Pandas. Images of the dataset have been resized to 32x32, and all of them are RGB color images. There are 34799 training images, 12630 validation validation images, and 4410 test images. There are 43 unique categories of traffic signs in the data. Below is the histogram showing size of each category in the training data, which indicates that the dataset is asymmetric. Some categories have more than 1000 examples, while others have less than 200 examples.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="../../../../images/2017/08/02/hist_raw.png"&gt;&lt;/p&gt;
&lt;p&gt;43 images was shown below with one arbitrary image from each category of the training image. The traffic signs in the train image have been pruned such that they are ovarally in similar size and in the center of the images. However, the intensity significantly varies from images to images, and some images were not taken precisely in the front view and some were not exactly aligned perpenticular to the ground.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/raw-class-42.png" width = 700&gt;&lt;/p&gt;
&lt;h3&gt;Data Augmentation and Preprocessing&lt;/h3&gt;
&lt;p&gt;Since the dataset is asymmetric and the size of each category is not sufficient for a complex ConvNet mode, data augmentation is required for this dataset to avoid overfitting of our model, and the augmentation can be completed by vertically and horizontally shifting, resizing, warping, and changing brightness of original training images with given parameter. Examples of shifting, resizing, image warping, changing brightness are shown below with the 20km speed limit sign.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/Demo_augmentations.png" width = 650&gt;&lt;/p&gt;
&lt;p&gt;My assumption is that the training data set reflects road conditions, and the road condition determines that certain traffic signs are met more frequently than others. Hence, the proportion of categories is maintained in my augmented data. The size of a categoriy in the augmented data is 7 times more than the size of the same category in the original data. For each categary, my function randomly picks an image from the original data, on which a random combination of changing brightness, horizontal and vertical shifting, rotation and resizing, and affine transform will be applied. Examples of images of augmentated data are plotted below.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/augm-class-42.png" width = 700&gt;&lt;/p&gt;
&lt;p&gt;Preprocessing is also required for images to be input to the neural network. The basic step is normalization. Other techniques, such as converting the original RGB images to grayscale or applying histogram equalization to images, have also been tested. However, no significant improvement has been observed with grascale or histogram equalized images. Thus, simple normalization is the only preprocessing applied in this study. The performance of my final model is obtained from image augmentation and the network architecture.&lt;/p&gt;
&lt;h3&gt;Model Architecture Design&lt;/h3&gt;
&lt;p&gt;The layer pattern of my ConvNet model is as follows&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Layer&lt;/th&gt;
&lt;th align="center"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Input&lt;/td&gt;
&lt;td align="center"&gt;32x32x3 RGB image&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 32x32x16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 32x32x32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Max pooling&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride,  outputs 16x16x32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 16x16x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 16x16x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Max pooling&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride,  outputs 8x8x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 8x8x128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, same padding, outputs 8x8x128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Max pooling&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride,  outputs 4x4x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;2048x2048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;2048x1024&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DROPOUT&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;1024x10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model includes six convolutional layers and three fully connected layers. Connected convolutional layers enables a wide view on the inputs for the model. Every convolutional layer and fully connected layer is followed by a dropout layer, which has effectively reduced overfitting. A pooling layer is used after every two convolutional layers, hence the size of feature maps are not shrinking too fast.&lt;/p&gt;
&lt;p&gt;The Adam optimizer was used to train this model. The learning rate is 2e-4 and the dropout probability is 0.5. A minibatch size of 128 is chosen for this model. The model achieves 97% validation accuracy in 70 epochs. The ccuracy on the pickled test data is 98%.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/ConvTrain.png" width = 650&gt;&lt;/p&gt;
&lt;p&gt;I started the architecture design with LeNet, because it is the very first successful ConvNet for image classification. I have achieved 89-91% validation accuracy with LeNet on the original training data. At the moment, the training accuracy quickly converged to 100%, which indicates overfitting. My first improvement was testing dropout layers at different depth of LeNet, which ends up with a dropout layer following every other layers. This improvement quickly help my model achieve 93-94% accuracy. Then with two more convolutional layers and image augmentation, I finalized my model to the current architecture.&lt;/p&gt;
&lt;h3&gt;Test a Model on New Images&lt;/h3&gt;
&lt;p&gt;Six test images were downloaded from the internet to test my model as shown below. All of the six images are relatively easy to be recognized, since they are in overall clearly taken. Although the first (true class id 11) and sixth (true class id 33) are not in the front view, but this have been incoorporated in the augmented data by applying affine transform. The prediction on the second (true class id 19) and fifth (true class id 41) are incorrect. This may be explained by the asymmetry of the augmented data, since the proportion of Class 19 and Class 41 are low. It may be inapproprate to keep the proportion of classes in the augmented data. My improvement will be training the model with symmetric augmented data.&lt;/p&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/test-imgs.png" width = 650&gt;&lt;/p&gt;
&lt;p&gt;The top five softmax probabilities for each test image by my model are listed below. It seems my model is very confident on its prediction. For correctly predicted images, the model is 100% sure on the best choice. Although probabilities of the best predictions are not 100% for incorrect predictions, the probabilities are greater than 99%. Referred to images of each class shown above, the top 5 predictions for incorrect predictions are very similar to the truth, e.g. red triangular frames are observed for all top 5 preditions for Image 3. Differences between those predictions exist in the center of the red triangle frame. In order to recognize these small scale differences, multiscale structures may be considered to improve my model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image 1: Right-of-way at the next intersection&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;100%&lt;/td&gt;
&lt;td align="center"&gt;Right-of-way at the next intersection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Pedestrians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;General caution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Beware of ice/snow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Ahead only&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Image 2: Dangerous curve to the left&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;99.92%&lt;/td&gt;
&lt;td align="center"&gt;Bicycles crossing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0.08%&lt;/td&gt;
&lt;td align="center"&gt;Road narrows on the right&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Dangerous curve to the left&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Road work&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Pedestrians&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Image 3: Speed limit (30km/h)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;100%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (30km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (20km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (50km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (70km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (80km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Image 4: Yield&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;100%&lt;/td&gt;
&lt;td align="center"&gt;Yield&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (120km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Beware of ice/snow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Keep right&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;General caution&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Image 5: End of no passing&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;99.8%&lt;/td&gt;
&lt;td align="center"&gt;Speed limit (30km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0.13%&lt;/td&gt;
&lt;td align="center"&gt;Road work (120km/h)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0.06%&lt;/td&gt;
&lt;td align="center"&gt;End of no passing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Priority road&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;End of no passing by vehicles over 3.5 metric tons&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Image 6: Turn right ahead&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Probability&lt;/th&gt;
&lt;th align="center"&gt;Prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;100%&lt;/td&gt;
&lt;td align="center"&gt;Turn right ahead&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Ahead only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Go straight or right&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Keep right&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;0%&lt;/td&gt;
&lt;td align="center"&gt;Roundabout mandatory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For the second image ... &lt;/p&gt;
&lt;h3&gt;(Optional) Visualizing the Neural Network&lt;/h3&gt;
&lt;p&gt;Eight arbitrary feature maps from the outputs of each convolutional layers are shown below. The observation is that the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.&lt;/p&gt;
&lt;h6&gt;Conv Layer 1&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_1.png" width = 650&gt;&lt;/p&gt;
&lt;h6&gt;Conv Layer 2&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_2.png" width = 650&gt;&lt;/p&gt;
&lt;h6&gt;Conv Layer 3&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_3.png" width = 650&gt;&lt;/p&gt;
&lt;h6&gt;Conv Layer 4&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_4.png" width = 650&gt;&lt;/p&gt;
&lt;h6&gt;Conv Layer 5&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_5.png" width = 650&gt;&lt;/p&gt;
&lt;h6&gt;Conv Layer 6&lt;/h6&gt;
&lt;p&gt;&lt;img src="../../../../images/2017/08/02/featMap_6.png" width = 650&gt;&lt;/p&gt;</content></entry><entry><title>Setting up a jupyter notebook server on paperspace</title><link href="/posts/2017/05/16/setting-up-a-jupyter-notebook-server-on-paperspace.html" rel="alternate"></link><published>2017-05-16T22:20:00-05:00</published><updated>2017-05-16T22:20:00-05:00</updated><author><name>By-the-water</name></author><id>tag:None,2017-05-16:/posts/2017/05/16/setting-up-a-jupyter-notebook-server-on-paperspace.html</id><summary type="html">&lt;p&gt;A brief tutorial on how to set up a Jupyter Notebook server on Paperspace. I have been using AWS for Kaggle and for problems of my personal interests. The AWS spot instances are convenient and affordable for individuals in most situations except for deep learning. The costs easily ramp up to $1.00 per hour when I ran a g2.2xlarge instance set up with the deep learning AMI and with sufficient storage. In this case, Paperspace is a good and affordable alternative to AWS.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been using AWS for Kaggle and for problems of my personal interests. The AWS spot instances are convenient and affordable for individuals in most situations except for deep learning. The costs easily ramp up to $1.00 per hour when I ran a g2.2xlarge instance set up with the deep learning AMI and with sufficient storage. In this case, Paperspace is a good and affordable alternative to AWS.&lt;/p&gt;
&lt;p&gt;Just like AWS, &lt;a href="https://paperspace.zendesk.com/hc/en-us/articles/216032308-What-is-Paperspace-"&gt;paperspace&lt;/a&gt; provides access to persistent cloud-based virtual machines, but at a much affordable prices. For machine learning applications, I can choose from two &lt;a href="https://www.paperspace.com/pricing"&gt;options&lt;/a&gt; at the time this post was written. The GPU+ option charges $0.4 per hour along with $5.00 for 50GB per month for the storage, and it comes with 8 CPUs, 30GB RAM, and a 8GB Quadro M4000 GPU. This is fairly sufficient for my purpose. Following this detailed &lt;a href="https://blog.paperspace.com/cpu-instances/"&gt;tutorial&lt;/a&gt;, we can easily set up a virtual machine with Ubuntu and major deep learning packages installed.&lt;/p&gt;
&lt;p&gt;By default, we can access both the desktop and the terminal of our virtual machine on our browser. This works very well. I feel no difference for using Jupyter Notebook in a virtual desktop within the browser on a 24' monitor. However, the font size and mouse response lag become a challenge on my Lenovo X1 Yoga. I think I still need a Jupyter Notebook server for the advantages of my eyes, and it turns to be contents in this post.&lt;/p&gt;
&lt;p&gt;You first need to assign a public IP to your virtual paperspace machine, then set up the Jupyter Notebook server as you are in AWS, and allow connections to your port in order to access your server in the browser.&lt;/p&gt;
&lt;p&gt;For those who are not very familiar with all these process, you can follow my brief tutorial as follows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assign Public IP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In your paperspace console, the Public IP can be assigned at
&lt;img alt="01" src="../../../../images/2017/05/16/01.PNG"&gt;
This is the IP you can connect to your virtual machine through ssh and browser.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting Jupyter Notebook Server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can set up the server following the &lt;a href="http://jupyter-notebook.readthedocs.io/en/latest/public_server.html"&gt;official doc&lt;/a&gt; of jupyter notebook as follows:&lt;/p&gt;
&lt;p&gt;Open a terminal in your console as shown in the following plot. you will be asked for the password to log in, which has been sent to your registered email.
&lt;img alt="02" src="../../../../images/2017/05/16/02.PNG"&gt;
If you don't have one already, generate a default notebook configuration file &lt;em&gt;~/.jupyter/jupyter_notebook_config.py&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:~$ jupyter notebook --generate-config
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Before modifying the notebook configuration, we need to generate a hashed password. As of notebook version 5.0, we can enter and store a password with a one-line command as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:~$ jupyter notebook password
Enter password: 
Verify password:
[NotebookPasswordApp] Wrote hashed password to /home/paperspace/.jupyter/jupyter_notebook_config.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a directory, and generate a self-signed certificate &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:~$ mkdir cert
:~$ cd cert
:~/cert$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mykey.key -out mycert.pem
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We need to set up the paths of our certificate files and the hashed password in the notebook configuration file. We can use Vi in a terminal or use whatever editor we like in the in-browser desktop to open the file &lt;em&gt;~/.jupyter/jupyter_notebook_config.py&lt;/em&gt;. We are going to add following settings to the beginning of the file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Set options for certfile, ip, password, and toggle off
# browser auto-opening
c.NotebookApp.certfile = u&amp;#39;/absolute/path/to/your/certificate/mycert.pem&amp;#39;
c.NotebookApp.keyfile = u&amp;#39;/absolute/path/to/your/certificate/mykey.key&amp;#39;
# Set ip to &amp;#39;*&amp;#39; to bind on all interfaces (ips) for the public server
c.NotebookApp.ip = &amp;#39;*&amp;#39;
c.NotebookApp.password = u&amp;#39;your hashed password in the .json file in this folder&amp;#39;
c.NotebookApp.open_browser = False

# It is a good idea to set a known, fixed port for server access
c.NotebookApp.port = [your choice of port number, e.g. 9999]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Save the file and exit from the editor. The notebook server is ready to run.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allow UFW port access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to access notebook server from our browser, we have to set UFW to allow us accessing the port of the notebook server. This can be done with the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo ufw allow [your notebook server port]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will re-enter our linux password at here.&lt;/p&gt;
&lt;p&gt;All set! Now use this address to access your server. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//[your public IP]:[your port]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If it is your first time seeing the following page, click 'ADVANCED' then 'Processd to ....', type in your password for the notebook server. Enjoy!
&lt;img alt="03" src="../../../../images/2017/05/16/03.PNG"&gt;&lt;/p&gt;</content></entry></feed>