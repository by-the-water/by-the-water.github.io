<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>  Behavioral cloning
 | By the water</title>

    <meta name="author" content="by-the-water"/>

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css"/>
    <link rel="stylesheet" href="/theme/css/jquery.mglass.css"/>
    <link rel="stylesheet" href="/theme/css/pygment-solarized-dark.css"/>
    <link rel="stylesheet" href="/theme/css/style.css"/>

    <!-- Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'/>
    <link href='https://fonts.googleapis.com/css?family=Istok+Web' rel='stylesheet' type='text/css'/>
    <link href='https://fonts.googleapis.com/css?family=Droid+Sans+Mono' rel='stylesheet' type='text/css'/>


    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <!-- Feeds -->
      <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="By the water - All posts - Atom Feed"/>
      <link href="/feeds/self-driving-cars.atom.xml" type="application/atom+xml" rel="alternate" title="By the water - Category: Self-driving Cars - Atom Feed"/>


  </head>

  <body>

    <div class="container">

      <div class="page-header">
        <h1><a href="">By the water</a> <small>Geoscience, Oil & Gas, AI</small></h1>
      </div>

      <nav class="navbar navbar-default">

        <!-- Hamburger menu for mobile -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#plumage-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="" title="Geoscience, Oil &amp; Gas, AI">By the water</a>
        </div>

        <!-- Menus and search forms -->
        <div class="collapse navbar-collapse" id="plumage-navbar-collapse-1">

          <ul class="nav navbar-nav">
<li >
                <a href="mailto:rossxsy@gmail.com">Email Me</a>
              </li>
<li >
                  <a href="/pages/about.html">About Me</a>
                </li>
<li >
                  <a href="/category/cloud.html">Cloud</a>
                </li>
<li  class="active" >
                  <a href="/category/self-driving-cars.html">Self-driving Cars <span class="sr-only">(current)</span></a>
                </li>
          </ul>



        </div>

      </nav>

    </div>


    <div class="container main">


      <div class="row">
        <div class=" col-md-9  ">
  <h1>
    <a href="/posts/2017/09/17/behavioral-cloning.html" rel="bookmark" title="Permalink to Behavioral cloning">Behavioral cloning</a>
  </h1>
        </div>
      </div>

      <div class="row">


        <div class=" col-md-9 " id="content" role="main">
  

  <div>
    <h1><strong>Udacity Self-driving Cars NanoDegree Project 3: Behavioral Cloning</strong></h1>
<h2>Summary</h2>
<p><strong>This is my work for the Behavioral Cloning project of the Udacity Self-driving Cars Nanodegree. A ConvNet-based model was built to autonomously predict steering of a simulated car. The architecture of this model is based on <a href="http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf">Nvidia's End to End Learning for Self-Driving Cars</a>. However, several adjustments have been made on the architecture to improve the performance on my dataset, and were documented in this documented. Although Udacity has provided some data, I decided to collect my own dataset using Udacity's <a href="https://github.com/udacity/self-driving-car-sim">car simulator</a>. In this project, I have generated a very small dataset of 2700 images on purpose, and augmented it with several data augmentation strategies to train my model. My model has been tested on the basic track provided in the simulator, indicating that data augmentatation can stretch the limit of a small dataset. Results of the model are shown below. This model does not make too far on the challenge track, which is longer and with more sharp turnings. Data augmentataion cannot help on this complex problem, and a much larger dataset are required.</strong></p>
<p><a href="https://youtu.be/Q-5QoDszvWQ"><img alt="Lake Track" src="../../../../images/2017/09/17/raw_center.png"></a>
<a href="https://youtu.be/Q-5QoDszvWQ">YouTube Link</a></p>
<hr>
<h2>Project Description</h2>
<p>This project includes:
<em> Data Collection and Preprocessing
</em> Data Augmentation
<em> Model Architecture
</em> Model Training
* Discussion</p>
<h3>Quick Start</h3>
<p>The environment for this project can be installed following Udacity's <a href="https://github.com/udacity/CarND-Term1-Starter-Kit">CarND Term1 Starter Kit</a>. Following files are used to build this model:
<em> model.py: build the model
</em> drive.py: run a pretrained model
<em> util.py: functions defined for preprocessing and image augmentation
</em> model.h5: my pretrained model</p>
<p><strong>Run the pretrained model</strong>
Start Udacity's car simulator, choose the basic track and the Autonomous Mode. The simulated car will be ready in the track. Then open a terminal and run the model using</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">drive</span><span class="o">.</span><span class="n">py</span> <span class="o">&lt;</span><span class="n">model</span> <span class="nb">file</span><span class="o">&gt;</span>
</pre></div>


<p>A preliminary versioin of drive.py was provided by Udacity, but I have added a line with image cropping.</p>
<p><strong>Train the model</strong>
To train the model, the path to the training data set must be set for the list <code>path</code> in <code>model.py</code>. Once path has been set, just run the model with</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">model</span><span class="o">.</span><span class="n">py</span>
</pre></div>


<hr>
<h3>Data Collection and Preprocessing</h3>
<p>We can collect data using the Training Mode of the simulator by capturing photos from three cameras set on the simulated car. The total numbers of images captured for each tracks are listed in the table below. This is a very small dataset, but I have tried to maximize the value of this dataset with image augmentation. This got the model to work on the basic track, but did not make too far on the challenge track with many sharp turnings.</p>
<table>
<thead>
<tr>
<th>Tracks</th>
<th>Number of Images</th>
</tr>
</thead>
<tbody>
<tr>
<td>Track 1</td>
<td>2787</td>
</tr>
</tbody>
</table>
<p>The steering angle of the center camera was recorded along with the photos. Examples of pictures of three cameras taken at the same time are shown below.</p>
<table>
<thead>
<tr>
<th><strong>Left Camera</strong></th>
<th><strong>Center Camera</strong></th>
<th><strong>Right Camera</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="alt text" src="../../../../images/2017/09/17/raw_left.png"></td>
<td><img alt="alt text" src="../../../../images/2017/09/17/raw_center.png"></td>
<td><img alt="alt text" src="../../../../images/2017/09/17/raw_right.png"></td>
</tr>
</tbody>
</table>
<p>I only collect photos fron three to four rounds on the track, since image augmentation will be applied. While driving on the track, I constantly changed the car from the border of the track to the center. More data steering the car back to the center were recorded in this way. The images have been cropped to focus on the road, and resized to reduce the dimension of the data. The cropped center image above is shown below. In addition, a normalization layer was set in the model to normalize the input image.</p>
<p><img src="../../../../images/2017/09/17/cropped_center.png" width = 500></p>
<h3>Data Augmentation</h3>
<p>Several strategies have been applied on the original images to increase the size of our samples and to generalize our data for cases such as sharp turnings. The strategies used are listed below.
* Use of images from the left and right camera. A image was randomly selected from the center, left, and right images captured at the same time. Since the images have been shifted, steering correction has been applied by adding 0.2 to the left image and subtracting 0.2 from the right image.</p>
<ul>
<li>The selected image was randomly flipped. The trained model tended to drive on the left side of the track without flipped data, since the data was collected while driving clockwise on the track.</li>
</ul>
<p><img src="../../../../images/2017/09/17/flipped_center.png" width = 500></p>
<ul>
<li>We randomly shifted the data horizontally and vertically. This strategy is equavallent to increasing more sharp turning data. The steering angle of a shifted image must be corrected. In this project, I assume that the correction is linearly correlated to the horizontal offset.</li>
</ul>
<p><img src="../../../../images/2017/09/17/translated_center.png" width = 500></p>
<ul>
<li>The brightness of the image has been added in order to avoid overfitting on the data.</li>
</ul>
<p><img src="../../../../images/2017/09/17/dim_center.png" width = 500></p>
<ul>
<li>A Gaussian noise has also been added to avoid overfitting.</li>
</ul>
<p><img src="../../../../images/2017/09/17/gNoise_center.png" width = 500></p>
<h3>Model Architecture</h3>
<p>My network is based on the architecture of <a href="https://github.com/udacity/CarND-Term1-Starter-Kit">Nvidia's End to End Learning for Self-Driving Cars</a>. However, several adjustments have been made in order to improve the results on my dataset.
<em> The input images were converted in HSV plane rather than YUV suggested in the paper
</em> The input image resolution is 64x200x3
<em> Maxpooling layers were used insteal of subsampling in convolutional layers
</em> Same padding was used instead of valid padding
<em> A dropout layer was added between the flattened layer to the fully-connected layers
</em> Numbers of parameters were increated in fully-connected layers, since I have more parameters out of the convolutional layers than the original architecture</p>
<p>My final architecture is documented below. Based on my experiments, it is important to ensure that no pooling or subsampling has been applied following the last two convolutional layers in order to get a significant improvement on the performance.</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Output Shape</th>
<th>Number</th>
<th>Connected to</th>
<th>Filter size</th>
</tr>
</thead>
<tbody>
<tr>
<td>lambda layer</td>
<td>(None, 64, 200,  3)</td>
<td>1</td>
<td>Input layer</td>
<td></td>
</tr>
<tr>
<td>Convolution2D</td>
<td>(None, 64, 200, 24)</td>
<td>2</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>MaxPooling2D</td>
<td>(None, 32, 100, 24)</td>
<td>3</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td>Convolution2D</td>
<td>(None, 32, 100, 36)</td>
<td>4</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>MaxPooling2D</td>
<td>(None, 16,  50, 36)</td>
<td>5</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>Convolution2D</td>
<td>(None, 16,  50, 48)</td>
<td>6</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>MaxPooling2D</td>
<td>(None,  8,  25, 48)</td>
<td>7</td>
<td>6</td>
<td></td>
</tr>
<tr>
<td>Convolution2D</td>
<td>(None,  8,  25, 64)</td>
<td>8</td>
<td>7</td>
<td>3</td>
</tr>
<tr>
<td>Convolution2D</td>
<td>(None,  8,  25, 64)</td>
<td>9</td>
<td>8</td>
<td>3</td>
</tr>
<tr>
<td>Flatten layer</td>
<td>(None, 12800)</td>
<td>10</td>
<td>9</td>
<td></td>
</tr>
<tr>
<td>Dropout</td>
<td>(None, 12800)</td>
<td>11</td>
<td>10</td>
<td></td>
</tr>
<tr>
<td>dense_1 (Dense)</td>
<td>(None, 1000)</td>
<td>12</td>
<td>11</td>
<td></td>
</tr>
<tr>
<td>dense_2 (Dense)</td>
<td>(None, 500)</td>
<td>13</td>
<td>12</td>
<td></td>
</tr>
<tr>
<td>dense_3 (Dense)</td>
<td>(None, 100)</td>
<td>14</td>
<td>13</td>
<td></td>
</tr>
<tr>
<td>dense_4 (Dense)</td>
<td>(None, 1)</td>
<td>15</td>
<td>14</td>
<td></td>
</tr>
</tbody>
</table>
<h3>Model Training</h3>
<p>Sine this dataset is small, I did not split it to training and validation set. The training and validation generators were both created with all of the images on the basic track. Since each of the image will be stochastically augmented, my assumption is that I can stochstically generate a traning set and a validation set with the generators. The training:validation ratio is 4:1. The model was tested on the simulator, and works for the simple basic track. However, this strategy only works for simple tracks. For the complex challenge track, much more data are required to train a valid model.</p>
<p>Hyperparameters for training are documented below:
<em> The training data size is chosen to be around 20000. This gives the best performance in combination with the batch size of 128
</em> My best model is obtained after 20 epochs
* <a href="https://arxiv.org/pdf/1412.6980v8.pdf">The Adaptive Momentum Estimation (Adam)</a> optimizer is chosen for the training. Adam updates every parameter in the model with an individual learning rate, which has simplified the tuning for learning rate. In the project, we chose a relatively fast learning rate 1e-3, and let Adam to adapt themselves through the training process. We don't need to worry about the learning rate decay with Adam. The training loss and validation loss are shown below. The initial training loss is very large, because I have assigned a high learning rate. Since Adam has adjusted every weight individually, training loss and validation loss are similar starting from the second epoch.</p>
<p><img src="../../../../images/2017/09/17/model_history.png" width = 500></p>
<h3>Discussion</h3>
<p>A small data set of the basic track have been used to train my networks with augmentation, and the model works on this track. However, using a small dataset for the challenge track, which is much longer and consists of more sharp turnings, does not generate a valid model. More data are required for this complex track.</p>
  </div>


<!--    -->
	<div id="disqus_thread"></div>
	<script type="text/javascript">
		var disqus_shortname = 'https-by-the-water-github-io';
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
        </div>

          <div class="col-md-3">
  <div class="well">

    <p><abbr title="2017-09-17T23:55:00-05:00"><i class="fa fa-calendar"></i> Sun 17 September 2017</abbr></p>

      <p><address>
        <i class="fa fa-user"></i> By
          <a href="/pages/about.html" rel="author">By-the-water</a>
      </address></p>

    <hr/>

      <p>
              <a href="/category/self-driving-cars.html" rel="tag"
                  data-toggle="tooltip" class="label label-info"
                  title="2 articles in this category">Self-driving Cars</a>
      </p>
      <hr/>



  </div>
            
          </div>

      </div>

    </div>

    <!-- TODO: make footer sticky -->
    <footer class="container-fluid">
      <div class="container">
        <div class="row">

            <div class="col-md-2">
            </div>
            <div class="col-md-2">
            </div>

          <div class="col-md-2">
            <h5>Browse content by</h5>
            <ul class="list-unstyled">
                <li><a href="/archives/index.html"><i class="fa fa-calendar"></i> Dates</a></li>
            </ul>
          </div>

          <div class="col-md-2 text-muted">
            <h5>Copyright notice</h5>
            <p>© Copyright 2017 by-the-water.</p>
          </div>

          <div class="col-md-2 text-muted">
            <h5>Disclaimer</h5>
              <p>All opinions expressed in this site are my own personal opinions and are not endorsed by, nor do they represent the opinions of my previous, current and future employers or any of its affiliates, partners or customers.</p>
          </div>

          <div class="col-md-2">
              <h5>Feeds</h5>
              <ul class="list-unstyled">
                  <li><small><a href="/feeds/all.atom.xml"><i class="fa fa-rss"></i> All posts (Atom)</a></small></li>
                  <li><small><a href="/feeds/self-driving-cars.atom.xml"><i class="fa fa-rss"></i> Category: Self-driving Cars (Atom)</a></small></li>
              </ul>
          </div>

        </div>
      </div>

      <h5 class="text-right"><a href="#"><i class="fa fa-arrow-up"></i> Back to top</a></h5>

      <div class="container">
        <div class="row col-md-12 text-muted text-center">
          Site generated by <a href="https://getpelican.com"> Pelican</a>.<br/>
          <a href="https://github.com/kdeldycke/plumage"> Plumage</a> theme by <a href="https://kevin.deldycke.com">Kevin Deldycke</a>.
        </div>
      </div>

    </footer>

    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-99319293-1', 'auto');
    ga('send', 'pageview');
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fitvids/1.1.0/jquery.fitvids.min.js"></script>
    <script src="/theme/js/jquery.mglass.js"></script>
    <script src="/theme/js/application.js"></script>

  </body>
</html>